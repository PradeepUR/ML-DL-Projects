{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1agea03Hk7zDRzaMBc_jFFzCHxBn09smF","timestamp":1736512174385}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"3DR-eO17geWu"},"source":["# Convolutional Neural Network"]},{"cell_type":"markdown","metadata":{"id":"EMefrVPCg-60"},"source":["### Importing the libraries"]},{"cell_type":"code","source":["# !pip install keras"],"metadata":{"id":"sxDJjguJRNhf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator"],"metadata":{"id":"GFrwhaDQN6VG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf.__version__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"y4RJ-_9nOb98","executionInfo":{"status":"ok","timestamp":1736682091424,"user_tz":-330,"elapsed":459,"user":{"displayName":"Pradeep U R","userId":"01174020656531879597"}},"outputId":"218bfa2e-0da1-4714-9b92-8abb45148af0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.17.1'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"oxQxCBWyoGPE"},"source":["## Part 1 - Data Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"MvE-heJNo3GG"},"source":["### Preprocessing the Training set"]},{"cell_type":"code","source":["# train_datagen object--> feature Scaling and transformation\n","train_datagen = ImageDataGenerator(rescale = 1./255, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)\n","\n","training_set = train_datagen.flow_from_directory('dataset/traininig_set', target_size = (64, 64), batch_size = 32, class_mode = 'binary')"],"metadata":{"id":"mQ6iuhpCRxNN","colab":{"base_uri":"https://localhost:8080/","height":339},"executionInfo":{"status":"error","timestamp":1736682103873,"user_tz":-330,"elapsed":625,"user":{"displayName":"Pradeep U R","userId":"01174020656531879597"}},"outputId":"4f5fe1a5-36af-44a7-9a23-ee27479ee691"},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'dataset/traininig_set'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-6169e4d07f45>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_datagen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrescale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshear_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzoom_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhorizontal_flip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtraining_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_datagen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow_from_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dataset/traininig_set'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'binary'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/legacy/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1136\u001b[0m         \u001b[0mkeep_aspect_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m     ):\n\u001b[0;32m-> 1138\u001b[0;31m         return DirectoryIterator(\n\u001b[0m\u001b[1;32m   1139\u001b[0m             \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/legacy/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset/traininig_set'"]}]},{"cell_type":"code","source":["# from google.colab import drive\n","# drive.mount('/content/drive')"],"metadata":{"id":"yyBh4TfJbi0A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mrCMmGw9pHys"},"source":["### Preprocessing the Test set"]},{"cell_type":"code","source":["test_datagen = ImageDataGenerator(rescale = 1./255)\n","test_set = test_datagen.flow_from_directory('dataset/test_set', target_size = (64, 64), batch_size = 32, class_mode = 'binary')"],"metadata":{"id":"sNbz6XEEb5Wv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"af8O4l90gk7B"},"source":["## Part 2 - Building the CNN"]},{"cell_type":"markdown","metadata":{"id":"ces1gXY2lmoX"},"source":["### Initialising the CNN"]},{"cell_type":"code","source":["cnn = tf.keras.models.Sequential()"],"metadata":{"id":"fJEcgR2Nc0vQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u5YJj_XMl5LF"},"source":["### Step 1 - Convolution"]},{"cell_type":"code","source":["cnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, activation = 'relu', input_shape = [64, 64, 3]))"],"metadata":{"id":"6oL3L92wc-3o","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736682116561,"user_tz":-330,"elapsed":610,"user":{"displayName":"Pradeep U R","userId":"01174020656531879597"}},"outputId":"8e93a4f5-85ea-4688-ab5f-9d8681cfebc6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]}]},{"cell_type":"markdown","metadata":{"id":"tf87FpvxmNOJ"},"source":["### Step 2 - Pooling"]},{"cell_type":"code","source":["cnn.add(tf.keras.layers.MaxPool2D(pool_size = (2, 2), strides = 2))"],"metadata":{"id":"rdNQODr-eC87"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xaTOgD8rm4mU"},"source":["### Adding a second convolutional layer"]},{"cell_type":"code","source":["cnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, activation = 'relu'))\n","cnn.add(tf.keras.layers.MaxPool2D(pool_size = (2, 2), strides = 2))"],"metadata":{"id":"NLzlr4rZe5i9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tmiEuvTunKfk"},"source":["### Step 3 - Flattening"]},{"cell_type":"code","source":["cnn.add(tf.keras.layers.Flatten())"],"metadata":{"id":"oXPzBzC29Fxe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dAoSECOm203v"},"source":["### Step 4 - Full Connection"]},{"cell_type":"code","source":["cnn.add(tf.keras.layers.Dense(units = 128, activation = 'relu'))"],"metadata":{"id":"SZ22KX_y9Mt9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yTldFvbX28Na"},"source":["### Step 5 - Output Layer"]},{"cell_type":"code","source":["cnn.add(tf.keras.layers.Dense(units = 2, activation = 'sigmoid'))"],"metadata":{"id":"52N00Hwa9lf0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D6XkI90snSDl"},"source":["## Part 3 - Training the CNN"]},{"cell_type":"markdown","metadata":{"id":"vfrFQACEnc6i"},"source":["### Compiling the CNN"]},{"cell_type":"code","source":["cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"],"metadata":{"id":"CMcjYP-y942E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ehS-v3MIpX2h"},"source":["### Training the CNN on the Training set and evaluating it on the Test set"]},{"cell_type":"code","source":["cnn.fit(x = training_set, validation_data = test_set, epochs = 25)"],"metadata":{"id":"T4z0r8tT-iEd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U3PZasO0006Z"},"source":["## Part 4 - Making a single prediction"]},{"cell_type":"code","source":["import numpy as np\n","from tensorflow.keras.preprocessing import image\n","test_image = image.load_img('dataset/simple_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n","# Converting pil format of images to an array\n","test_image = image.img_to_array(test_image)\n","# Adding extra dimension of the batch size(batch is the 1st dimension)\n","test_image = np.expand.dims(test_image, axis = 0)\n","result = cnn.predict(test_image)\n","training_set.class_indices\n","if result[0][0] == 1:\n","  prediction = 'dog'\n","else:\n","  prediction = 'cat'"],"metadata":{"id":"ZIa9e0YJ_B4X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cnn.fit"],"metadata":{"id":"IKJQiNbaAZRQ"},"execution_count":null,"outputs":[]}]}